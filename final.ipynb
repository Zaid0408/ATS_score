{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions below to extract data from resumes using text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract name from resume\n",
    "def extract_name_from_resume(text):\n",
    "    name = None\n",
    "\n",
    "    # Use regex pattern to find a potential name\n",
    "    pattern = r\"(\\b[A-Z][a-z]+\\b)\\s(\\b[A-Z][a-z]+\\b)\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        name = match.group()\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract email and phone number and any urls from resume\n",
    "def extract_email_phone(text):\n",
    "    email_regex = r'[A-Za-z0-9\\._%+\\-]+@[A-Za-z0-9\\.\\-]+\\.[A-Za-z]{2,}'\n",
    "    phone_regex = r'\\d{3}-\\d{3}-\\d{4}|\\(\\d{3}\\)\\s\\d{3}-\\d{4}|\\d{3}\\.\\d{3}\\.\\d{4}|\\d{3}\\s\\d{3}\\s\\d{4}|\\d{10}|\\d{5}\\s\\d{5}|\\+\\d{2}\\s\\d{10}|\\+\\d{2}\\s\\d{5}\\s\\d{5}'\n",
    "\n",
    "    emails = re.findall(email_regex, text)\n",
    "    phones = re.findall(phone_regex, text)\n",
    "    urls= re.findall(r'\\b((?:https?:\\/\\/|www\\.)(?:[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.)+[a-zA-Z]{2,6}\\b(?:[-a-zA-Z0-9@:%_\\+.~#?&//=]*))', text)\n",
    "    \n",
    "    processed_phones = []\n",
    "    for phone in phones:\n",
    "        if len(phone) > 10:\n",
    "            phone = phone[-10:]  # Keep the last 10 digits\n",
    "        processed_phones.append(phone)\n",
    "\n",
    "    linkedin_regex = r'^https?://(?:www\\.)?linkedin\\.com/[^\\s]+|linkedin\\.com/in/[\\w-]+'\n",
    "    linkedin_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        if re.match(linkedin_regex, url):\n",
    "            linkedin_urls.append(url)\n",
    "    \n",
    "    if not linkedin_urls:\n",
    "        linkedin_urls = [\"None\"]\n",
    "\n",
    "    return emails, processed_phones, urls, linkedin_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_education_from_resume(text):\n",
    "    education = []\n",
    "    pattern = r\"(?i)(?:(?:Bachelor|B\\.S\\.|B\\.A\\.|Master|M\\.S\\.|M\\.A\\.|Ph\\.D\\.)\\s(?:[A-Za-z]+\\s)*[A-Za-z]+)|(?:Bsc|\\bB\\.\\w+|\\bM\\.\\w+|\\bPh\\.D\\.\\w+|\\bBachelor(?:'s)?|\\bMaster(?:'s)?|\\bPh\\.D)\\s(?:\\w+\\s)*\\w+\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    for match in matches:\n",
    "        education.append(match.strip())\n",
    "\n",
    "    return education\n",
    "\n",
    "def extract_college_name(text):\n",
    "    college_pattern = r'(?i)(?:[A-Z][a-z]* College of Engineering|[A-Z][a-z]* Educational Institute|University of [A-Z][a-z]*|Ecole [A-Z][a-z]*|Indian Institute Of Technology-[A-Z][a-z]|National Institute Of Technology-[A-Z][a-z])'\n",
    "\n",
    "  # Use re.findall to find all matches in the entire text\n",
    "    matches = re.findall(college_pattern, text)\n",
    "\n",
    "  # Return the first match if found, otherwise return None\n",
    "    return matches[0].strip() if matches else None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below code is to extract data from resumes using normal basic functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been processed and written to dataofresume.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('resumes.csv')\n",
    "\n",
    "# Create a list to store the processed data\n",
    "processed_data = []\n",
    "resume_data = df['Resume txt'].tolist()\n",
    "\n",
    "# Process each resume text\n",
    "for index, row in df.iterrows():\n",
    "    sl_no = row['Sl no']\n",
    "    text = row['Resume txt']\n",
    "    \n",
    "    emails, phones, urls, linkedIn_url = extract_email_phone(text)\n",
    "    name = extract_name_from_resume(text)\n",
    "    edu_details = extract_education_from_resume(text)\n",
    "    \n",
    "    processed_data.append({\n",
    "        \"sl no\": sl_no,\n",
    "        \"Name\": name,\n",
    "        \"emails\": ', '.join(emails),\n",
    "        \"contact no's\": ', '.join(phones),\n",
    "        \"LinkedIn\": ', '.join(linkedIn_url),\n",
    "        \"Education\": ', '.join(edu_details)\n",
    "    })\n",
    "\n",
    "# Create a new DataFrame from the processed data\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Write the processed data to dataofresume.csv\n",
    "processed_df.to_csv('dataofresume.csv', index=False)\n",
    "\n",
    "print(\"Data has been processed and written to dataofresume.csv\")\n",
    "# print(resume_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "job_desc=read_txt_file('sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuations\n",
    "    word_tokens = [word for word in word_tokens if word.isalnum()]\n",
    "\n",
    "    # Remove stopwords and common words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Add more common words to remove if needed\n",
    "    common_words = set(['from', 'what', 'and'])\n",
    "    word_tokens = [word for word in word_tokens if not word in stop_words and not word in common_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ml', 'code', 'unsupervised', 'excellent', 'algorithm', 'quality', 'develop', 'life', 'phd', 'understand', 'passionate', 'azure', 'environment', 'development', 'job', 'candidate', 'work', 'similar', 'datasets', 'interested', 'practice', 'requirement', 'experience', 'mission', 'hadoop', 'professional', 'version', 'look', 'pipeline', 'project', 'framework', 'implement', 'transform', 'late', 'scalable', 'innovative', 'role', 'program', 'analytical', 'publish', 'support', 'career', 'evaluation', 'problem', 'strong', 'solid', 'vision', 'analyze', 'familiarity', 'ability', 'relevant', 'ca', 'specialize', 'teamwork', 'deploy', 'prefer', 'employee', 'salary', 'improve', 'review', 'employer', 'remote', 'participate', 'learn', 'identify', 'package', 'offer', 'commit', 'machine', 'extract', 'training', 'production', 'supervise', 'make', 'benefit', 'model', 'hour', 'letter', 'processing', 'performance', 'insight', 'cover', 'related', 'create', 'apply', 'collaborative', 'location', 'san', 'computer', 'software', 'integrate', 'detail', 'skill', 'monitor', 'evaluate', 'kafka', 'tensorflow', 'nlp', 'competitive', 'language', 'inclusive', 'motivation', 'big', 'francisco', 'drive', 'electrical', 'engineering', 'natural', 'communication', 'cloud', 'google', 'research', 'qualification', 'engineer', 'resume', 'application', 'ensure', 'field', 'meaningful', 'u', 'complex', 'conduct', 'celebrate', 'solution', 'finding', 'tech', 'various', 'bachelor', 'paper', 'invite', 'master', 'ai', 'best', 'knowledge', 'technique', 'solve', 'degree', 'aws', 'spark', 'talented', 'responsibility', 'lead', 'maintain', 'team', 'improvement', 'plus', 'include', 'continuous', 'proven', 'join', 'scalability', 'optimize', 'feature', 'technology', 'submit', 'current', 'accuracy', 'flexible', 'diversity', 'opportunity', 'deployed', 'pytorch', 'option', 'preprocessing', 'python', 'advancement', 'collaborate', 'description', 'design', 'data', 'contribution', 'industry', 'company', 'large', 'necessary', 'dynamic', 'equal', 'stay', 'science', 'platform', 'usa', 'control'}\n"
     ]
    }
   ],
   "source": [
    "job_desc_processed = preprocess(job_desc)\n",
    "job_desc_skills = set(job_desc_processed.split())\n",
    "print(job_desc_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = resume_data+ [job_desc]\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents into TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the TF-IDF matrix to an array\n",
    "tfidf_array = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances to the nearest resumes: [[0.49717068 0.51415742 0.51638184 0.52133094 0.54598075 0.63196613\n",
      "  0.64834057 0.65652105 0.70806493 0.78081552 0.79934059 0.83724388]]\n",
      "Indices of the nearest resumes: [[ 0  7  6  9 11  1  2 10  3  5  4  8]]\n",
      "ATS Scores: [[50.28293238 48.58425844 48.3618157  47.86690615 45.40192535 36.80338747\n",
      "  35.16594272 34.34789453 29.19350714 21.91844846 20.06594144 16.27561163]]\n"
     ]
    }
   ],
   "source": [
    "resume_vectors = tfidf_array[:-1]  # All except the last\n",
    "job_desc_vector = tfidf_array[-1]  # The last vector\n",
    "\n",
    "# Create and train the KNN model on resume vectors\n",
    "knn_model = NearestNeighbors(n_neighbors=12, metric='cosine')\n",
    "knn_model.fit(resume_vectors)\n",
    "\n",
    "# Find the k most similar resumes to the job description vector\n",
    "distances, indices = knn_model.kneighbors(job_desc_vector.reshape(1, -1))\n",
    "ats_scores = (1 - distances) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Distances to the nearest resumes:\", distances)\n",
    "print(\"Indices of the nearest resumes:\", indices)\n",
    "print(\"ATS Scores:\", ats_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12,)\n",
      "(12,)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "[50.28293238 36.80338747 35.16594272 29.19350714 20.06594144 21.91844846\n",
      " 48.3618157  48.58425844 16.27561163 47.86690615 34.34789453 45.40192535]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "indices=np.array(indices)\n",
    "ats_scores=np.array(ats_scores)\n",
    "indices=indices.reshape(-1)\n",
    "ats_scores=ats_scores.reshape(-1)\n",
    "\n",
    "\n",
    "n = len(indices)\n",
    "for i in range(n):\n",
    "    for j in range(0, n - i - 1):\n",
    "            # Swap if the element found is greater than the next element\n",
    "        if indices[j] > indices[j + 1]:\n",
    "            indices[j], indices[j + 1] = indices[j + 1], indices[j]\n",
    "            ats_scores[j], ats_scores[j + 1] = ats_scores[j + 1], ats_scores[j]\n",
    "\n",
    "\n",
    "print(indices.shape)\n",
    "print(ats_scores.shape)\n",
    "\n",
    "print(indices)\n",
    "print(ats_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN scores have been added successfully.\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.read_csv('dataofResume.csv')\n",
    "\n",
    "if 'KNN Score' not in new_df.columns:\n",
    "    new_df['KNN Score'] = None\n",
    "\n",
    "\n",
    "\n",
    "for i, index in enumerate(indices):\n",
    "    sl_no = index + 1  # Add 1 to match the Sl no in the CSV file\n",
    "    score = ats_scores[i]\n",
    "    new_df.loc[df['Sl no'] == sl_no, 'KNN Score'] = score\n",
    "\n",
    "# Save the updated CSV\n",
    "new_df.to_csv('dataOfResume.csv', index=False)\n",
    "\n",
    "print(\"KNN scores have been added successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume 1: 50.28% match with job description\n",
      "Resume 2: 36.8% match with job description\n",
      "Resume 3: 35.17% match with job description\n",
      "Resume 4: 29.19% match with job description\n",
      "Resume 5: 20.07% match with job description\n",
      "Resume 6: 21.92% match with job description\n",
      "Resume 7: 48.36% match with job description\n",
      "Resume 8: 48.58% match with job description\n",
      "Resume 9: 16.28% match with job description\n",
      "Resume 10: 47.87% match with job description\n",
      "Resume 11: 34.35% match with job description\n",
      "Resume 12: 45.4% match with job description\n"
     ]
    }
   ],
   "source": [
    "job_desc_vector = tfidf_array[-1].reshape(1, -1)  # The last vector, reshaped\n",
    "\n",
    "# Calculate cosine similarities\n",
    "similarities = cosine_similarity(resume_vectors, job_desc_vector)\n",
    "\n",
    "# Calculate percentage matches\n",
    "percentage_matches = [round(similarity[0] * 100, 2) for similarity in similarities]\n",
    "\n",
    "# Print the results\n",
    "for i, percentage_match in enumerate(percentage_matches):\n",
    "    print(f\"Resume {i + 1}: {percentage_match}% match with job description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity scores have been added successfully.\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.read_csv('dataofResume.csv')\n",
    "\n",
    "if 'Cosine Similarity' not in new_df.columns:\n",
    "    new_df['Cosine Similarity'] = None\n",
    "    \n",
    "new_df['Cosine Similarity'] = percentage_matches\n",
    "new_df.to_csv('dataOfResume.csv', index=False)\n",
    "\n",
    "print(\"Cosine Similarity scores have been added successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
